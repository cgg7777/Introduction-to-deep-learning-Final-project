{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"20180180_task_2.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ps5P2lDYkc_a","colab_type":"text"},"source":["[Wide-ResNet MixMatch](#scrollTo=NMzmcxDzLgdv&line=3&uniqifier=1)\n","\n","[U_net MixMatch](#scrollTo=zPibROeofH1D&line=10&uniqifier=1)\n","\n","[ResNet18 MixMatch](#scrollTo=JYqDLKwkwbdS&line=9&uniqifier=1)\n","\n","[Wide-ResNet MixMatch + increasing epoch + increasing early stopping count + decreasing learning rate](#scrollTo=9gDzmZ03yrDs&line=7&uniqifier=1)\n","\n","\n","[Wide-ResNet MixMatch + increasing epoch + increasing early stopping count + decreasing learning rate + Weight Decay ](#scrollTo=EtxH5QRz4M4X&line=4&uniqifier=1)"]},{"cell_type":"code","metadata":{"id":"CicoqVnILgdi","colab_type":"code","outputId":"a97eba03-cf7c-4958-9d71-10f4c7dc89e8","executionInfo":{"status":"ok","timestamp":1576837858436,"user_tz":-540,"elapsed":18830,"user":{"displayName":"밈뭄멈뭄","photoUrl":"","userId":"02132713927266936024"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["from __future__ import print_function\n","\n","!pip install progress\n","\n","import easydict\n","import os\n","import shutil\n","import time\n","import random\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","\n","\n","import os\n","import sys\n","# 다른 py 파일을 import하기 위한 code 입니다. Google Drive 상에서 작동하게 하기 위한 코드로, /Colab Notebooks/20180180_task_2'가 제 환경에서의 개인적인 경로입니다.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","sys.path.insert(0, '/content/gdrive/My Drive/Colab Notebooks/final project test/20180180_task_2')\n","\n","\n","# 아래 import 파일들은 모두 제공된 skeleton 입니다.\n","import models.wideresnet as models\n","import models.loss as loss\n","import dataset.cifar10 as dataset\n","from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: progress in /usr/local/lib/python3.6/dist-packages (1.5)\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"diX2unHkLgdl","colab_type":"code","outputId":"3a295ff6-0182-463d-cafe-a5af6b8be127","executionInfo":{"status":"ok","timestamp":1576837864836,"user_tz":-540,"elapsed":2833,"user":{"displayName":"밈뭄멈뭄","photoUrl":"","userId":"02132713927266936024"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 주어진 for_student.ipynb 의 기본 skeleton입니다.\n","\n","## This is argument.\n","args = easydict.EasyDict({\n","    \"epochs\": 16,   # number of total epochs to run\n","    \"batch_size\": 64, # train batchsize\n","    \"lr\": 0.002,       # initial learning rate\n","    \"resume\": '',     # path to latest checkpoint (default: none)\n","    \"gpu\": 0,\n","    \"T\" : 0.5,\n","    \"alpha\" : 0.75,\n","    \"n_labeled\": 250 , # Number of labeled data # please use 250 , 1000, 4000, 10000, 40000\n","    \"val_iteration\": 1024, # period of computing validation error\n","    \"out\": '', # Directory to output the result\n","    # You can add any configuration parameters for your own design!!\n","})\n","\n","print(args)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{'epochs': 16, 'batch_size': 64, 'lr': 0.002, 'resume': '', 'gpu': 0, 'T': 0.5, 'alpha': 0.75, 'n_labeled': 250, 'val_iteration': 1024, 'out': ''}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"135Z1vbeLgdp","colab_type":"code","colab":{}},"source":["# 주어진 for_student.ipynb의 기본 skeleton 에서 일부를 수정하였습니다.\n","# interleave 함수를 제거하고,  mixmatch를 수행하기 위한 함수를 정의하였습니다.\n","# mixmatch 함수는 https://github.com/gan3sh500/mixmatch-pytorch/blob/master/notebook.ipynb 의 코드를 바탕으로\n","# 일부 수정하여 완성하였습니다.\n","\n","# mixmatch 함수를 기존 skeleton에 맞게 사용하기 위해 parameter로 unlabeled data를 합치치 않은 채로\n","# 2 set을 전달받고, 이를 torch.cat으로 합친 후 qb를 계산하였습니다.\n","\n","def sharpen(x, T = 0.5):   # Sharpening을 수행하는 함수\n","  pt = x**(1/T)\n","  target_u = pt/pt.sum(dim=1,keepdim=True)\n","  target_u = target_u.detach()\n","  return target_u\n","\n","def mixup(x1, p1, x2, p2, a = 0.75):  # mixup을 수행하는 함수\n","  Lambda = np.random.beta(a, a)\n","  Lambda = max(Lambda, 1 - Lambda)  \n","\n","  x = Lambda * x1 + (1 - Lambda) * x2\n","  y = Lambda * p1 + (1 - Lambda) * p2\n","  return x, y\n","\n","def mixmatch(xb, y, inputs_u, inputs_u2, model, K=2):    # K의 default값을 2로 지정하였음\n","    outputs_u = model(inputs_u)\n","    outputs_u2 = model(inputs_u2)\n","    ub = torch.cat([inputs_u, inputs_u2], dim = 0)\n","    p = (outputs_u + outputs_u2) / 2\n","    qb = sharpen(p)\n","    Ux = ub\n","    Uy = torch.cat([qb for _ in range(K)], dim=0)\n","\n","    indices = np.random.shuffle(np.arange(len(xb) + len(Ux)))\n","    Wx = torch.cat([Ux, xb], axis=0)[indices]\n","    Wy = torch.cat([Uy, y], axis=0)[indices]\n","\n","    X, p = mixup(xb, y, Wx[0][:len(xb)], Wy[0][:len(xb)])\n","\n","    U, q = mixup(ub, Uy, Wx[0][len(xb):], Wy[0][len(xb):])\n","\n","    return X, p, U, q\n","\n","\n","def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, criterion, epoch, use_cuda=True):\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    losses_x = AverageMeter()\n","    losses_u = AverageMeter()\n","    ws = AverageMeter()\n","    end = time.time()\n","\n","    bar = Bar('Training', max=args.val_iteration)\n","    labeled_train_iter = iter(labeled_trainloader)\n","    unlabeled_train_iter = iter(unlabeled_trainloader)\n","\n","    model.train()\n","    for batch_idx in range(args.val_iteration):\n","        try:\n","            inputs_x, targets_x = labeled_train_iter.next()\n","        except:\n","            labeled_train_iter = iter(labeled_trainloader)\n","            inputs_x, targets_x = labeled_train_iter.next()\n","\n","        try:\n","            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n","        except:\n","            unlabeled_train_iter = iter(unlabeled_trainloader)\n","            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n","\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        batch_size = inputs_x.size(0)\n","\n","        # Transform label to one-hot\n","        targets_x = torch.zeros(batch_size, 10).scatter_(1, targets_x.view(-1,1), 1)\n","\n","        if use_cuda:\n","            inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n","            inputs_u = inputs_u.cuda()\n","            inputs_u2 = inputs_u2.cuda()\n","       \n","        # 아래 과정은 MixMatch가 일어나는 과정으로, 위에서 정의한 mixmatch, mixup, sharpen 함수를 사용합니다.\n","        with torch.no_grad():\n","            # compute guessed labels of unlabel samples\n","            outputs_u = model(inputs_u)\n","            outputs_u2 = model(inputs_u2)\n","\n","        unlabeled_target = torch.cat([outputs_u, outputs_u2], dim=0)\n","        all_inputs_u = torch.cat([inputs_u, inputs_u2])\n","        all_outputs_u = torch.cat([outputs_u, outputs_u2], dim = 0)\n","\n","\n","        mix_input, mix_target, mix_U, mix_p = mixmatch(inputs_x, targets_x, inputs_u, inputs_u2, model)\n","\n","        logits_x = model(mix_input)\n","        logits_u = model(mix_U)\n","\n","        Lx, Lu, w = criterion(logits_x, mix_target, logits_u, mix_p)\n","\n","\n","        loss = Lx + w * Lu\n","\n","        # record loss\n","        losses.update(loss.item(), inputs_x.size(0))\n","        losses_x.update(Lx.item(), inputs_x.size(0))\n","        losses_u.update(Lu.item(), inputs_x.size(0))\n","        ws.update(w, inputs_x.size(0))\n","\n","\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        # plot progress\n","\n","        if(batch_idx % 100 ==0):\n","          print('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f}'.format(\n","                      batch=batch_idx + 1,\n","                      size=args.val_iteration,\n","                      data=data_time.avg,\n","                      bt=batch_time.avg,\n","                      total=bar.elapsed_td,\n","                      loss=losses.avg,\n","                      loss_x=losses_x.avg,\n","                      loss_u=losses_u.avg,\n","                      ))\n","          bar.next()\n","    bar.finish()\n","\n","    return (losses.avg, losses_x.avg, losses_u.avg,)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_YLkU2mLgdr","colab_type":"code","colab":{}},"source":["# 주어진 for_student.ipynb의 validation function에 해당하는 기본 skeleton 입니다.\n","\n","def validate(valloader, model, criterion, epoch, use_cuda, mode):\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    top5 = AverageMeter()\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    end = time.time()\n","    bar = Bar(f'{mode}', max=len(valloader))\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(valloader):\n","            # measure data loading time\n","            data_time.update(time.time() - end)\n","\n","            if use_cuda:\n","                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n","\n","            # compute output\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            # measure accuracy and record loss\n","            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n","            losses.update(loss.item(), inputs.size(0))\n","            top1.update(prec1.item(), inputs.size(0))\n","            top5.update(prec5.item(), inputs.size(0))\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            # plot progress\n","            if((batch_idx+1 ==len(valloader))):\n","              print('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n","                          batch=batch_idx + 1,\n","                          size=len(valloader),\n","                          data=data_time.avg,\n","                          bt=batch_time.avg,\n","                          total=bar.elapsed_td,\n","                          loss=losses.avg,\n","                          top1=top1.avg,\n","                          top5=top5.avg,\n","                          ))\n","              bar.next()\n","        bar.finish()\n","    return (losses.avg, top1.avg)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"urURwVzPLgdt","colab_type":"code","outputId":"ef70a1fa-e080-46c4-e4a1-0b52d6a3d71e","executionInfo":{"status":"ok","timestamp":1576837867906,"user_tz":-540,"elapsed":1847,"user":{"displayName":"밈뭄멈뭄","photoUrl":"","userId":"02132713927266936024"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 주어진 for_student.ipynb의 기본 skeleton 입니다.\n","# Dataset을 로드해오는 기능을 수행합니다.\n","if not os.path.isdir(args.out):\n","    '''make dir if not exist'''\n","    try:\n","        os.makedirs(args.out)\n","    except:\n","        pass\n","\n","# Data\n","print(f'==> Preparing cifar10')\n","transform_train = transforms.Compose([\n","    dataset.RandomPadandCrop(32),\n","    dataset.RandomFlip(),\n","    dataset.ToTensor(),\n","])\n","\n","transform_val = transforms.Compose([\n","    dataset.ToTensor(),\n","])\n","\n","\n","\n","def CustomDataLoader(labeled_count):\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = dataset.get_cifar10('./data', labeled_count, transform_train=transform_train, transform_val=transform_val)\n","  return train_labeled_set, train_unlabeled_set, val_set, test_set\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["==> Preparing cifar10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NMzmcxDzLgdv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"166041a1-e6f2-4474-a924-edc50ae800c2"},"source":["# Wide_RestNet을 labeled data가 250, 1000, 4000, 10000개 있을 때의 훈련을 진행하여\n","# 각 case의 performance를 측정한다.\n","\n","def switch_state(x):   # for 문의 index를 통해 labeled data가 몇 개가 선택되어야 하는지를 결정하는 함수\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = models.WideResNet(num_classes=10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 3\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, args.epochs):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, args['lr']))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:    # 첫 epoch에서\n","        temp_val_loss = val_loss    # 임시 validation loss 변수에 이번 epoch의 validation loss를 저장한다.\n","      else:\n","        if (temp_val_loss < val_loss):   # 임시 저장해놓은 validation loss보다 이번 epoch의 validation loss가 더 클 경우\n","          early_stopping_count = early_stopping_count+1    # early_stopping count를 1 증가시킨다.\n","        else:\n","          best_epoch = epoch+1                      # 만일 validation loss가 작아졌다면, best_epoch 기록을 남기고\n","          temp_val_loss = val_loss                  # 임시 validation loss를 초기화하고\n","          early_stopping_count = 0                  # early_stopping count를 초기화한다.\n","\n","      if(early_stopping_count == early_stopping_standard):           # early_stopping count가 제한 횟수와 같아질 경우\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))               # print하고 학습을 종료한다.\n","        break;\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["250개의 labeled data를 이용하여 training 시작\n","\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["170500096it [00:06, 25610614.48it/s]                               \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Files already downloaded and verified\n","#Labeled: 250 #Unlabeled: 44750 #Val: 5000\n","==> creating backbone network\n","    Total params: 1.47M\n","==> defining loss function and optimizer\n","\n","Epoch: [1 | 16] LR: 0.002000\n","(1/1024) Data: 0.054s | Batch: 0.913s | Total: 0:00:00 | Loss: 2.9769 | Loss_x: 2.3381 | Loss_u: 0.0085\n","(101/1024) Data: 0.020s | Batch: 0.157s | Total: 0:00:15 | Loss: 2.8608 | Loss_x: 2.1894 | Loss_u: 0.0090\n","(201/1024) Data: 0.020s | Batch: 0.154s | Total: 0:00:30 | Loss: 2.7769 | Loss_x: 2.1094 | Loss_u: 0.0089\n","(301/1024) Data: 0.020s | Batch: 0.152s | Total: 0:00:45 | Loss: 2.7042 | Loss_x: 2.0270 | Loss_u: 0.0090\n","(401/1024) Data: 0.020s | Batch: 0.152s | Total: 0:01:00 | Loss: 2.5874 | Loss_x: 1.9121 | Loss_u: 0.0090\n","(501/1024) Data: 0.020s | Batch: 0.151s | Total: 0:01:15 | Loss: 2.5005 | Loss_x: 1.8295 | Loss_u: 0.0089\n","(601/1024) Data: 0.020s | Batch: 0.151s | Total: 0:01:30 | Loss: 2.4179 | Loss_x: 1.7528 | Loss_u: 0.0089\n","(701/1024) Data: 0.020s | Batch: 0.151s | Total: 0:01:45 | Loss: 2.3432 | Loss_x: 1.6776 | Loss_u: 0.0089\n","(801/1024) Data: 0.020s | Batch: 0.151s | Total: 0:02:00 | Loss: 2.2937 | Loss_x: 1.6275 | Loss_u: 0.0089\n","(901/1024) Data: 0.020s | Batch: 0.151s | Total: 0:02:15 | Loss: 2.2420 | Loss_x: 1.5810 | Loss_u: 0.0088\n","(1001/1024) Data: 0.020s | Batch: 0.151s | Total: 0:02:30 | Loss: 2.1907 | Loss_x: 1.5331 | Loss_u: 0.0088\n","(3/3) Data: 0.006s | Batch: 0.028s | Total: 0:00:00 | Loss: 0.5734 | top1:  88.0208 | top5:  99.4792\n","(79/79) Data: 0.001s | Batch: 0.012s | Total: 0:00:00 | Loss: 1.7901 | top1:  44.3200 | top5:  84.3400\n","(157/157) Data: 0.001s | Batch: 0.012s | Total: 0:00:01 | Loss: 1.8213 | top1:  43.9900 | top5:  84.1000\n","\n","Epoch: [2 | 16] LR: 0.002000\n","(1/1024) Data: 0.023s | Batch: 0.137s | Total: 0:00:00 | Loss: 1.7682 | Loss_x: 0.9648 | Loss_u: 0.0107\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.5760 | Loss_x: 0.9522 | Loss_u: 0.0083\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.5798 | Loss_x: 0.9576 | Loss_u: 0.0083\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.5940 | Loss_x: 0.9747 | Loss_u: 0.0083\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.6058 | Loss_x: 0.9945 | Loss_u: 0.0082\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.6117 | Loss_x: 1.0031 | Loss_u: 0.0081\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.6015 | Loss_x: 1.0010 | Loss_u: 0.0080\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.5873 | Loss_x: 0.9899 | Loss_u: 0.0080\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.5831 | Loss_x: 0.9893 | Loss_u: 0.0079\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.5798 | Loss_x: 0.9879 | Loss_u: 0.0079\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:29 | Loss: 1.5774 | Loss_x: 0.9863 | Loss_u: 0.0079\n","(3/3) Data: 0.007s | Batch: 0.027s | Total: 0:00:00 | Loss: 0.5738 | top1:  87.5000 | top5:  100.0000\n","(79/79) Data: 0.000s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.5939 | top1:  50.5400 | top5:  84.9200\n","(157/157) Data: 0.000s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.5982 | top1:  50.0200 | top5:  85.7700\n","\n","Epoch: [3 | 16] LR: 0.002000\n","(1/1024) Data: 0.024s | Batch: 0.138s | Total: 0:00:00 | Loss: 1.8565 | Loss_x: 1.3100 | Loss_u: 0.0073\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.5826 | Loss_x: 0.9979 | Loss_u: 0.0078\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.5952 | Loss_x: 1.0074 | Loss_u: 0.0078\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.5444 | Loss_x: 0.9769 | Loss_u: 0.0076\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.5324 | Loss_x: 0.9725 | Loss_u: 0.0075\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.5348 | Loss_x: 0.9724 | Loss_u: 0.0075\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.5095 | Loss_x: 0.9456 | Loss_u: 0.0075\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.5055 | Loss_x: 0.9482 | Loss_u: 0.0074\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.5055 | Loss_x: 0.9508 | Loss_u: 0.0074\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.5196 | Loss_x: 0.9579 | Loss_u: 0.0075\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:29 | Loss: 1.5191 | Loss_x: 0.9610 | Loss_u: 0.0074\n","(3/3) Data: 0.007s | Batch: 0.027s | Total: 0:00:00 | Loss: 0.4228 | top1:  96.3542 | top5:  100.0000\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.5506 | top1:  51.3400 | top5:  85.6800\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.5610 | top1:  50.7800 | top5:  84.9300\n","\n","Epoch: [4 | 16] LR: 0.002000\n","(1/1024) Data: 0.024s | Batch: 0.138s | Total: 0:00:00 | Loss: 0.8030 | Loss_x: 0.3718 | Loss_u: 0.0057\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.4544 | Loss_x: 0.9180 | Loss_u: 0.0072\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.4418 | Loss_x: 0.9038 | Loss_u: 0.0072\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.4904 | Loss_x: 0.9548 | Loss_u: 0.0071\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.4846 | Loss_x: 0.9514 | Loss_u: 0.0071\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.4842 | Loss_x: 0.9500 | Loss_u: 0.0071\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.4842 | Loss_x: 0.9495 | Loss_u: 0.0071\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.4782 | Loss_x: 0.9476 | Loss_u: 0.0071\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.4939 | Loss_x: 0.9613 | Loss_u: 0.0071\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.4909 | Loss_x: 0.9590 | Loss_u: 0.0071\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.4893 | Loss_x: 0.9600 | Loss_u: 0.0071\n","(3/3) Data: 0.007s | Batch: 0.027s | Total: 0:00:00 | Loss: 0.3797 | top1:  98.9583 | top5:  100.0000\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.4676 | top1:  56.3000 | top5:  83.0200\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.4581 | top1:  56.4900 | top5:  83.3700\n","\n","Epoch: [5 | 16] LR: 0.002000\n","(1/1024) Data: 0.022s | Batch: 0.136s | Total: 0:00:00 | Loss: 0.6863 | Loss_x: 0.1438 | Loss_u: 0.0072\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.4186 | Loss_x: 0.9103 | Loss_u: 0.0068\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.4432 | Loss_x: 0.9130 | Loss_u: 0.0071\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.4849 | Loss_x: 0.9443 | Loss_u: 0.0072\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.4727 | Loss_x: 0.9336 | Loss_u: 0.0072\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.4697 | Loss_x: 0.9343 | Loss_u: 0.0071\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.4632 | Loss_x: 0.9329 | Loss_u: 0.0071\n","(701/1024) Data: 0.021s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.4642 | Loss_x: 0.9331 | Loss_u: 0.0071\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.4540 | Loss_x: 0.9281 | Loss_u: 0.0070\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.4583 | Loss_x: 0.9332 | Loss_u: 0.0070\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.4627 | Loss_x: 0.9365 | Loss_u: 0.0070\n","(3/3) Data: 0.007s | Batch: 0.027s | Total: 0:00:00 | Loss: 0.4083 | top1:  97.9167 | top5:  100.0000\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.3621 | top1:  58.5400 | top5:  85.4600\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.3916 | top1:  57.5000 | top5:  85.3400\n","\n","Epoch: [6 | 16] LR: 0.002000\n","(1/1024) Data: 0.024s | Batch: 0.138s | Total: 0:00:00 | Loss: 1.0911 | Loss_x: 0.2610 | Loss_u: 0.0111\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.3040 | Loss_x: 0.8066 | Loss_u: 0.0066\n","(201/1024) Data: 0.021s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.3441 | Loss_x: 0.8430 | Loss_u: 0.0067\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.3590 | Loss_x: 0.8644 | Loss_u: 0.0066\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.3746 | Loss_x: 0.8837 | Loss_u: 0.0065\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.3777 | Loss_x: 0.8888 | Loss_u: 0.0065\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.3894 | Loss_x: 0.8960 | Loss_u: 0.0066\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.3950 | Loss_x: 0.9003 | Loss_u: 0.0066\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.3986 | Loss_x: 0.9015 | Loss_u: 0.0066\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.4007 | Loss_x: 0.9026 | Loss_u: 0.0066\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.4010 | Loss_x: 0.9041 | Loss_u: 0.0066\n","(3/3) Data: 0.007s | Batch: 0.027s | Total: 0:00:00 | Loss: 0.4041 | top1:  98.9583 | top5:  100.0000\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.5558 | top1:  53.6200 | top5:  87.0800\n","(157/157) Data: 0.000s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.5830 | top1:  52.9700 | top5:  86.8200\n","\n","Epoch: [7 | 16] LR: 0.002000\n","(1/1024) Data: 0.021s | Batch: 0.135s | Total: 0:00:00 | Loss: 1.5803 | Loss_x: 1.2401 | Loss_u: 0.0045\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.3736 | Loss_x: 0.8814 | Loss_u: 0.0066\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.3975 | Loss_x: 0.8882 | Loss_u: 0.0068\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.3965 | Loss_x: 0.8892 | Loss_u: 0.0068\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.3847 | Loss_x: 0.8897 | Loss_u: 0.0066\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.3788 | Loss_x: 0.8878 | Loss_u: 0.0065\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.3796 | Loss_x: 0.8919 | Loss_u: 0.0065\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.3743 | Loss_x: 0.8917 | Loss_u: 0.0064\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.3634 | Loss_x: 0.8842 | Loss_u: 0.0064\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.3559 | Loss_x: 0.8797 | Loss_u: 0.0063\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.3566 | Loss_x: 0.8798 | Loss_u: 0.0064\n","(3/3) Data: 0.006s | Batch: 0.027s | Total: 0:00:00 | Loss: 0.4233 | top1:  96.3542 | top5:  100.0000\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.4879 | top1:  55.9800 | top5:  83.1600\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.4961 | top1:  55.7800 | top5:  82.6600\n","\n","Epoch: [8 | 16] LR: 0.002000\n","(1/1024) Data: 0.021s | Batch: 0.135s | Total: 0:00:00 | Loss: 2.0323 | Loss_x: 1.6451 | Loss_u: 0.0052\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.3112 | Loss_x: 0.8289 | Loss_u: 0.0064\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.3477 | Loss_x: 0.8859 | Loss_u: 0.0062\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.3219 | Loss_x: 0.8706 | Loss_u: 0.0060\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.3364 | Loss_x: 0.8837 | Loss_u: 0.0060\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.3443 | Loss_x: 0.8855 | Loss_u: 0.0061\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.3548 | Loss_x: 0.8903 | Loss_u: 0.0062\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.3544 | Loss_x: 0.8932 | Loss_u: 0.0061\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.3564 | Loss_x: 0.8969 | Loss_u: 0.0061\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.3567 | Loss_x: 0.8981 | Loss_u: 0.0061\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.3404 | Loss_x: 0.8864 | Loss_u: 0.0061\n","(3/3) Data: 0.007s | Batch: 0.028s | Total: 0:00:00 | Loss: 0.4282 | top1:  95.3125 | top5:  100.0000\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.5206 | top1:  55.3000 | top5:  86.2400\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.5402 | top1:  54.6600 | top5:  85.0500\n","최적 epoch : 5\n","\n","1000개의 labeled data를 이용하여 training 시작\n","\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","#Labeled: 1000 #Unlabeled: 44000 #Val: 5000\n","==> creating backbone network\n","    Total params: 1.47M\n","==> defining loss function and optimizer\n","\n","Epoch: [1 | 16] LR: 0.002000\n","(1/1024) Data: 0.021s | Batch: 0.131s | Total: 0:00:00 | Loss: 4.5217 | Loss_x: 2.5321 | Loss_u: 0.0265\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 2.9016 | Loss_x: 2.2748 | Loss_u: 0.0084\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 2.8640 | Loss_x: 2.2393 | Loss_u: 0.0083\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 2.8345 | Loss_x: 2.2200 | Loss_u: 0.0082\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 2.8386 | Loss_x: 2.1982 | Loss_u: 0.0085\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 2.8258 | Loss_x: 2.1799 | Loss_u: 0.0086\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 2.8182 | Loss_x: 2.1584 | Loss_u: 0.0088\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 2.8069 | Loss_x: 2.1486 | Loss_u: 0.0088\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 2.7914 | Loss_x: 2.1295 | Loss_u: 0.0088\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 2.7689 | Loss_x: 2.1064 | Loss_u: 0.0088\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 2.7464 | Loss_x: 2.0825 | Loss_u: 0.0089\n","(15/15) Data: 0.007s | Batch: 0.020s | Total: 0:00:00 | Loss: 1.5906 | top1:  45.7292 | top5:  89.4792\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.7422 | top1:  39.2400 | top5:  84.6200\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.7387 | top1:  39.2000 | top5:  84.9600\n","\n","Epoch: [2 | 16] LR: 0.002000\n","(1/1024) Data: 0.021s | Batch: 0.135s | Total: 0:00:00 | Loss: 2.3817 | Loss_x: 1.7840 | Loss_u: 0.0080\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 2.4694 | Loss_x: 1.8162 | Loss_u: 0.0087\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 2.4300 | Loss_x: 1.7615 | Loss_u: 0.0089\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 2.4132 | Loss_x: 1.7395 | Loss_u: 0.0090\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 2.3770 | Loss_x: 1.7104 | Loss_u: 0.0089\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 2.3451 | Loss_x: 1.6851 | Loss_u: 0.0088\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 2.3070 | Loss_x: 1.6569 | Loss_u: 0.0087\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 2.2863 | Loss_x: 1.6388 | Loss_u: 0.0086\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 2.2613 | Loss_x: 1.6172 | Loss_u: 0.0086\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 2.2288 | Loss_x: 1.5916 | Loss_u: 0.0085\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 2.1987 | Loss_x: 1.5657 | Loss_u: 0.0084\n","(15/15) Data: 0.006s | Batch: 0.019s | Total: 0:00:00 | Loss: 1.0651 | top1:  70.6250 | top5:  95.9375\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.4423 | top1:  55.0600 | top5:  86.3400\n","(157/157) Data: 0.000s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.4505 | top1:  54.0400 | top5:  86.1600\n","\n","Epoch: [3 | 16] LR: 0.002000\n","(1/1024) Data: 0.020s | Batch: 0.134s | Total: 0:00:00 | Loss: 2.8521 | Loss_x: 1.9921 | Loss_u: 0.0115\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.9735 | Loss_x: 1.3487 | Loss_u: 0.0083\n","(201/1024) Data: 0.019s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.9538 | Loss_x: 1.3528 | Loss_u: 0.0080\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.9236 | Loss_x: 1.3384 | Loss_u: 0.0078\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.9251 | Loss_x: 1.3278 | Loss_u: 0.0080\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.8959 | Loss_x: 1.3087 | Loss_u: 0.0078\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.8959 | Loss_x: 1.3052 | Loss_u: 0.0079\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.8901 | Loss_x: 1.2939 | Loss_u: 0.0079\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.8741 | Loss_x: 1.2795 | Loss_u: 0.0079\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.8668 | Loss_x: 1.2729 | Loss_u: 0.0079\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.8657 | Loss_x: 1.2717 | Loss_u: 0.0079\n","(15/15) Data: 0.007s | Batch: 0.019s | Total: 0:00:00 | Loss: 0.8624 | top1:  84.7917 | top5:  98.8542\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.4268 | top1:  58.5800 | top5:  87.1800\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.4188 | top1:  58.8600 | top5:  87.2400\n","\n","Epoch: [4 | 16] LR: 0.002000\n","(1/1024) Data: 0.022s | Batch: 0.137s | Total: 0:00:00 | Loss: 2.4349 | Loss_x: 2.0132 | Loss_u: 0.0056\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.7498 | Loss_x: 1.1472 | Loss_u: 0.0080\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.7127 | Loss_x: 1.1378 | Loss_u: 0.0077\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.6979 | Loss_x: 1.1206 | Loss_u: 0.0077\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.7095 | Loss_x: 1.1433 | Loss_u: 0.0076\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.7090 | Loss_x: 1.1432 | Loss_u: 0.0075\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.7204 | Loss_x: 1.1502 | Loss_u: 0.0076\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.7306 | Loss_x: 1.1640 | Loss_u: 0.0076\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.7136 | Loss_x: 1.1531 | Loss_u: 0.0075\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.7059 | Loss_x: 1.1485 | Loss_u: 0.0074\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.6967 | Loss_x: 1.1408 | Loss_u: 0.0074\n","(15/15) Data: 0.007s | Batch: 0.020s | Total: 0:00:00 | Loss: 0.6907 | top1:  95.5208 | top5:  99.6875\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.4486 | top1:  61.1600 | top5:  83.2000\n","(157/157) Data: 0.000s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.4435 | top1:  61.6600 | top5:  83.8200\n","\n","Epoch: [5 | 16] LR: 0.002000\n","(1/1024) Data: 0.021s | Batch: 0.135s | Total: 0:00:00 | Loss: 2.5440 | Loss_x: 1.9625 | Loss_u: 0.0078\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.7850 | Loss_x: 1.2174 | Loss_u: 0.0076\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.6876 | Loss_x: 1.1378 | Loss_u: 0.0073\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.6413 | Loss_x: 1.0821 | Loss_u: 0.0075\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.6527 | Loss_x: 1.1024 | Loss_u: 0.0073\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.6328 | Loss_x: 1.0857 | Loss_u: 0.0073\n","(601/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:30 | Loss: 1.6402 | Loss_x: 1.0963 | Loss_u: 0.0073\n","(701/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:45 | Loss: 1.6245 | Loss_x: 1.0822 | Loss_u: 0.0072\n","(801/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:00 | Loss: 1.6212 | Loss_x: 1.0855 | Loss_u: 0.0071\n","(901/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:15 | Loss: 1.6079 | Loss_x: 1.0744 | Loss_u: 0.0071\n","(1001/1024) Data: 0.020s | Batch: 0.150s | Total: 0:02:30 | Loss: 1.6042 | Loss_x: 1.0689 | Loss_u: 0.0071\n","(15/15) Data: 0.007s | Batch: 0.020s | Total: 0:00:00 | Loss: 0.6551 | top1:  97.3958 | top5:  100.0000\n","(79/79) Data: 0.001s | Batch: 0.011s | Total: 0:00:00 | Loss: 1.4656 | top1:  62.4000 | top5:  84.7400\n","(157/157) Data: 0.001s | Batch: 0.011s | Total: 0:00:01 | Loss: 1.4638 | top1:  62.6600 | top5:  85.1700\n","\n","Epoch: [6 | 16] LR: 0.002000\n","(1/1024) Data: 0.023s | Batch: 0.137s | Total: 0:00:00 | Loss: 2.5063 | Loss_x: 1.4626 | Loss_u: 0.0139\n","(101/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:15 | Loss: 1.4521 | Loss_x: 0.8702 | Loss_u: 0.0078\n","(201/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:30 | Loss: 1.5089 | Loss_x: 0.9573 | Loss_u: 0.0074\n","(301/1024) Data: 0.020s | Batch: 0.150s | Total: 0:00:45 | Loss: 1.5076 | Loss_x: 0.9720 | Loss_u: 0.0071\n","(401/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:00 | Loss: 1.5259 | Loss_x: 0.9906 | Loss_u: 0.0071\n","(501/1024) Data: 0.020s | Batch: 0.150s | Total: 0:01:15 | Loss: 1.5103 | Loss_x: 0.9911 | Loss_u: 0.0069\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zPibROeofH1D","colab_type":"code","colab":{}},"source":["# U_net model 정의\n","# https://github.com/usuyama/pytorch-unet/blob/master/pytorch_unet.py 에 기재된 U_net code를 일부\n","# 수정하여 사용하였습니다.\n","# CIFAR10 훈련에 사용하기 위해 output layer를 변경하였습니다.\n","\n","def double_conv(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True)\n","    )   \n","\n","\n","class UNet(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","                \n","        self.dconv_down1 = double_conv(3, 64)\n","        self.dconv_down2 = double_conv(64, 128)\n","        self.dconv_down3 = double_conv(128, 256)\n","        self.dconv_down4 = double_conv(256, 512)        \n","\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n","        \n","        self.dconv_up3 = double_conv(256 + 512, 256)\n","        self.dconv_up2 = double_conv(128 + 256, 128)\n","        self.dconv_up1 = double_conv(128 + 64, 64)\n","        \n","        self.conv_last = nn.Conv2d(64, 1, 1)\n","        self.fc = nn.Linear(32*32, 100)\n","        self.fc2 = nn.Linear (100, 10)\n","        \n","        \n","    def forward(self, x):\n","        conv1 = self.dconv_down1(x)\n","        x = self.maxpool(conv1)\n","\n","        conv2 = self.dconv_down2(x)\n","        x = self.maxpool(conv2)\n","        \n","        conv3 = self.dconv_down3(x)\n","        x = self.maxpool(conv3)   \n","        \n","        x = self.dconv_down4(x)\n","        \n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv3], dim=1)\n","        \n","        x = self.dconv_up3(x)\n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv2], dim=1)       \n","\n","        x = self.dconv_up2(x)\n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv1], dim=1)   \n","        \n","        x = self.dconv_up1(x)\n","        \n","        x = self.conv_last(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        out = self.fc2(x)\n","        \n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2NhPQjVuGpf","colab_type":"code","colab":{}},"source":["\"\"\"\n","위와 동일하게 학습 알고리즘을 가지나,\n","model만 Unet으로 교체하여 진행하였다.\n","\"\"\"\n","torch.cuda.empty_cache()\n","\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = UNet()\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 3\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, args.epochs):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, args['lr']))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYqDLKwkwbdS","colab_type":"code","colab":{}},"source":["\"\"\"\n","위와 동일하게 학습 알고리즘을 가지나,\n","model만 resnet18로 변경하여 학습하였다.\n","\"\"\"\n","import torchvision\n","torch.cuda.empty_cache()\n","\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = torchvision.models.resnet18(pretrained=False)\n","  model.fc = nn.Linear(512,10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 3\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, args.epochs):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, args['lr']))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gDzmZ03yrDs","colab_type":"code","colab":{}},"source":["# 위의 Wide-ResNet training과 동일하나, Epoch와 Early stopping 기준 count 를 증가시키고 performance를 체크하였다.\n","# 또한 learning rate를 감소시켰다.\n","torch.cuda.empty_cache()\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = models.WideResNet(num_classes=10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 5\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, 50):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, 50, 0.001))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtxH5QRz4M4X","colab_type":"code","colab":{}},"source":["# 위 Wide-ResNet에 3가지 변화를 준 모델에 L2 Legularization을 위해 weight decay를 추가하였다\n","# decay 상수값은 0.001로 설정하였다. \n","torch.cuda.empty_cache()\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = models.WideResNet(num_classes=10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.001)  # weight decay 적용\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 5\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, 50):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, 50, 0.001))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUgKtCJKoUUp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}